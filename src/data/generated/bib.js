const generatedBibEntries = {
    "Chen2017MV3D": {
        "abstract": "This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory\u2011fusion framework that takes both LiDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi\u2011view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi\u2011view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's\u2011eye\u2011view representation of 3D point cloud. We design a deep fusion scheme to combine region\u2011wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state of the art by around 25\u202f% and\u202f30\u202f% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3\u202f% higher AP than the state of the art on the hard set among LiDAR\u2011based methods.",
        "author": "Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian",
        "doi": "10.1109/CVPR.2017.690",
        "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
        "keywords": "3D object detection, sensor fusion, autonomous driving",
        "number": "1",
        "publisher": "IEEE",
        "series": "CVPR",
        "title": "Multi\u2011View 3D Object Detection Network for Autonomous Driving",
        "type": "article",
        "url": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf",
        "volume": "2017",
        "year": "2017"
    },
    "Lang2019PointPillars": {
        "abstract": "Object detection in point clouds is crucial for robotics applications such as autonomous driving. We introduce PointPillars, a novel encoder that employs PointNets to learn a representation of point clouds organized in vertical columns (pillars). The learned features can be used with any standard 2D convolutional detector; we pair them with a lean downstream network. PointPillars outperforms previous encoders in both speed and accuracy by large margins: it runs at 62\u202fHz (105\u202fHz in a faster variant) while achieving state\u2011of\u2011the\u2011art 3D and bird\u2019s\u2011eye\u2011view AP on the KITTI benchmark using only LiDAR.",
        "author": "Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar",
        "doi": "10.1109/CVPR.2019.00916",
        "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
        "keywords": "LiDAR, real\u2011time detection, 3D object detection, point cloud",
        "number": "1",
        "publisher": "IEEE",
        "series": "CVPR",
        "title": "PointPillars: Fast Encoders for Object Detection from Point Clouds",
        "type": "article",
        "url": "https://arxiv.org/abs/1812.05784",
        "volume": "2019",
        "year": "2019"
    },
    "Qi2018Frustum": {
        "abstract": "We study 3D object detection from RGB\u2011D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds popped up from RGB\u2011D scans. A key challenge is how to efficiently localize objects in large\u2011scale point clouds. Instead of relying solely on 3D proposals, our method leverages mature 2D detectors together with 3D deep learning, achieving efficiency and high recall even for small objects. Benefiting from learning directly in raw point clouds, our network precisely estimates 3D boxes under heavy occlusion or sparse points. On the KITTI and SUN RGB\u2011D benchmarks it outperforms the state of the art by large margins and runs in real time.",
        "author": "Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.",
        "doi": "10.1109/CVPR.2018.00959",
        "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
        "keywords": "3D object detection, point cloud, RGB\u2011D, deep learning",
        "number": "1",
        "publisher": "IEEE",
        "series": "CVPR",
        "title": "Frustum PointNets for 3D Object Detection from RGB\u2011D Data",
        "type": "inproceedings",
        "url": "https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Frustum_PointNets_for_CVPR_2018_paper.html",
        "volume": "2018",
        "year": "2018"
    },
    "Shi2019PointRCNN": {
        "abstract": "We introduce PointRCNN for 3D object detection from raw point clouds. The two\u2011stage framework first generates bottom\u2011up 3D proposals by segmenting the whole scene into foreground and background points, then refines each proposal in canonical coordinates by combining local spatial features with global semantic features. Evaluated on the KITTI 3D benchmark, PointRCNN achieves state\u2011of\u2011the\u2011art performance using only point clouds as input, demonstrating the effectiveness of bottom\u2011up proposal generation and canonical refinement.",
        "author": "Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng",
        "doi": "10.1109/CVPR.2019.00912",
        "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
        "keywords": "3D detection, LiDAR, point cloud segmentation, proposal generation",
        "number": "1",
        "publisher": "IEEE",
        "series": "CVPR",
        "title": "PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud",
        "type": "article",
        "url": "https://arxiv.org/abs/1812.04244",
        "volume": "2019",
        "year": "2019"
    },
    "Shi2020PVRCNN": {
        "abstract": "We present PointVoxel\u2011RCNN (PV\u2011RCNN), a high\u2011performance 3D object detector that deeply integrates voxel CNNs and PointNet\u2011based set abstraction to learn discriminative point\u2011cloud features. PV\u2011RCNN leverages efficient voxel feature learning for high\u2011quality proposals and flexible point\u2011wise receptive fields for accurate refinement. On KITTI and Waymo datasets, it surpasses prior LiDAR\u2011based methods by significant margins.",
        "author": "Shi, Shaoshuai and Guo, Chaoxu and Jiang, Li and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng",
        "doi": "10.1109/CVPR42600.2020.00476",
        "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "keywords": "3D detection, point\u2011voxel fusion, LiDAR",
        "number": "1",
        "publisher": "IEEE",
        "series": "CVPR",
        "title": "PV\u2011RCNN: Point\u2011Voxel Feature Set Abstraction for 3D Object Detection",
        "type": "article",
        "url": "https://arxiv.org/abs/1912.13192",
        "volume": "2020",
        "year": "2020"
    },
    "Shi2020PointGNN": {
        "abstract": "We propose Point\u2011GNN, a graph neural network for detecting objects in LiDAR point clouds. Points are encoded into a fixed\u2011radius nearest\u2011neighbor graph; Point\u2011GNN predicts object category and shape for each vertex. An auto\u2011registration mechanism reduces translation variance, while a box\u2011merging operation combines detections from multiple vertices. Point\u2011GNN achieves leading accuracy on KITTI using point cloud alone and even surpasses fusion methods.",
        "author": "Shi, Weijing and Rajkumar, Ragunathan",
        "doi": "10.1109/CVPR42600.2020.00215",
        "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "keywords": "graph neural networks, 3D detection, point cloud",
        "number": "1",
        "publisher": "IEEE",
        "series": "CVPR",
        "title": "Point\u2011GNN: Graph Neural Network for 3D Object Detection in a Point Cloud",
        "type": "article",
        "url": "https://arxiv.org/abs/2003.01251",
        "volume": "2020",
        "year": "2020"
    },
    "Vora2020PointPainting": {
        "abstract": "LiDAR and camera provide complementary information for self\u2011driving cars, yet LiDAR\u2011only detectors still dominate benchmarks. We propose PointPainting: a sequential fusion scheme that projects LiDAR points into the output of an image\u2011only semantic\u2011segmentation network and appends class scores to each point. The painted point cloud feeds any LiDAR detector. Large improvements are demonstrated on Point\u2011RCNN, VoxelNet, and PointPillars across KITTI and nuScenes.",
        "author": "Vora, Sourabh and Lang, Alex H. and Helou, Bassam and Beijbom, Oscar",
        "doi": "10.1109/CVPR42600.2020.00538",
        "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "keywords": "sensor fusion, semantic segmentation, LiDAR, camera",
        "number": "1",
        "publisher": "IEEE",
        "series": "CVPR",
        "title": "PointPainting: Sequential Fusion for 3D Object Detection",
        "type": "article",
        "url": "https://arxiv.org/abs/1911.10150",
        "volume": "2020",
        "year": "2020"
    },
    "Wang2020PseudoLiDAR": {
        "abstract": "3D object detection is essential for autonomous driving. State\u2011of\u2011the\u2011art accuracy relies on precise but expensive LiDAR sensors, while cheaper monocular or stereo imagery yields far lower accuracies. Contrary to common belief that poor depth estimation is the culprit, we argue that representation is the key. We convert image\u2011based depth maps into pseudo\u2011LiDAR points, mimicking LiDAR signals so that off\u2011the\u2011shelf LiDAR detectors can be applied. On KITTI, our approach boosts stereo\u2011image 3D detection accuracy within 30\u202fm from 22\u202f% to 74\u202f%, topping the leaderboard for image\u2011based methods at submission time.",
        "author": "Wang, Yan and Chao, Wei\u2011Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.",
        "doi": "10.1109/CVPR42600.2020.00811",
        "journal": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "keywords": "pseudo\u2011LiDAR, stereo depth, monocular depth, 3D detection",
        "number": "1",
        "publisher": "IEEE",
        "series": "CVPR",
        "title": "Pseudo\u2011LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving",
        "type": "article",
        "url": "https://arxiv.org/abs/1812.07179",
        "volume": "2020",
        "year": "2020"
    },
    "Wang2021FCOS3D": {
        "abstract": "Monocular 3D object detection is attractive for autonomous driving due to its low cost but is challenging because depth is missing. Building on a fully convolutional single\u2011stage detector, we propose FCOS3D. By transforming 3D targets into the image domain, distributing objects across feature levels by 2D scale, and redefining center\u2011ness with a 2D Gaussian around the projected 3D center, FCOS3D becomes a simple yet effective framework without 2D\u20113D priors. It ranked 1st among vision\u2011only methods on the nuScenes 3D detection challenge (NeurIPS\u00a02020).",
        "author": "Wang, Tai and Zhu, Xinge and Pang, Jiangmiao and Lin, Dahua",
        "doi": "10.1109/ICCV48922.2021.00912",
        "journal": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
        "keywords": "monocular 3D detection, anchor\u2011free, autonomous driving",
        "number": "1",
        "publisher": "IEEE",
        "series": "ICCV",
        "title": "FCOS3D: Fully Convolutional One\u2011Stage Monocular 3D Object Detection",
        "type": "article",
        "url": "https://arxiv.org/abs/2104.10956",
        "volume": "2021",
        "year": "2021"
    },
    "Zhou2018VoxelNet": {
        "abstract": "Accurate detection of objects in 3D point clouds is central to many applications, e.g., autonomous navigation and AR/VR. Prior work uses hand\u2011crafted projections such as bird's\u2011eye view to interface sparse LiDAR points with region\u2011proposal networks. We remove manual feature engineering and present VoxelNet, an end\u2011to\u2011end 3D detection network that unifies feature extraction and bounding\u2011box prediction. VoxelNet partitions the space into equally spaced 3D voxels and transforms the points inside each voxel into a unified feature via a novel voxel feature encoding layer. The encoded volumetric representation is processed by a region\u2011proposal network to generate detections. On the KITTI car benchmark, VoxelNet surpasses state\u2011of\u2011the\u2011art LiDAR\u2011only methods by a large margin.",
        "author": "Zhou, Yin and Tuzel, \\\"{O}ncel",
        "doi": "10.1109/CVPR.2018.00117",
        "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
        "keywords": "voxelization, LiDAR, 3D detection, deep learning",
        "number": "1",
        "publisher": "IEEE",
        "series": "CVPR",
        "title": "VoxelNet: End\u2011to\u2011End Learning for Point Cloud Based 3D Object Detection",
        "type": "article",
        "url": "https://arxiv.org/abs/1711.06396",
        "volume": "2018",
        "year": "2018"
    }
};