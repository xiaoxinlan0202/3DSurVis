@article{Chen2017MV3D,
  abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory‑fusion framework that takes both LiDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi‑view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi‑view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's‑eye‑view representation of 3D point cloud. We design a deep fusion scheme to combine region‑wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state of the art by around 25 % and 30 % AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3 % higher AP than the state of the art on the hard set among LiDAR‑based methods.},
  author    = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
  doi       = {10.1109/CVPR.2017.690},
  journal   = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {3D object detection, sensor fusion, autonomous driving},
  number    = {1},
  publisher = {IEEE},
  volume    = {2017},
  series    = {CVPR},
  title     = {Multi‑View 3D Object Detection Network for Autonomous Driving},
  url       = {https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf},
  year      = {2017}
}

@inproceedings{Qi2018Frustum,
  abstract = {We study 3D object detection from RGB‑D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds popped up from RGB‑D scans. A key challenge is how to efficiently localize objects in large‑scale point clouds. Instead of relying solely on 3D proposals, our method leverages mature 2D detectors together with 3D deep learning, achieving efficiency and high recall even for small objects. Benefiting from learning directly in raw point clouds, our network precisely estimates 3D boxes under heavy occlusion or sparse points. On the KITTI and SUN RGB‑D benchmarks it outperforms the state of the art by large margins and runs in real time.},
  author    = {Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
  doi       = {10.1109/CVPR.2018.00959},
  journal   = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {3D object detection, point cloud, RGB‑D, deep learning},
  number    = {1},
  publisher = {IEEE},
  volume    = {2018},
  series    = {CVPR},
  title     = {Frustum PointNets for 3D Object Detection from RGB‑D Data},
  url       = {https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Frustum_PointNets_for_CVPR_2018_paper.html},
  year      = {2018}
}

@article{Zhou2018VoxelNet,
  abstract = {Accurate detection of objects in 3D point clouds is central to many applications, e.g., autonomous navigation and AR/VR. Prior work uses hand‑crafted projections such as bird's‑eye view to interface sparse LiDAR points with region‑proposal networks. We remove manual feature engineering and present VoxelNet, an end‑to‑end 3D detection network that unifies feature extraction and bounding‑box prediction. VoxelNet partitions the space into equally spaced 3D voxels and transforms the points inside each voxel into a unified feature via a novel voxel feature encoding layer. The encoded volumetric representation is processed by a region‑proposal network to generate detections. On the KITTI car benchmark, VoxelNet surpasses state‑of‑the‑art LiDAR‑only methods by a large margin.},
  author    = {Zhou, Yin and Tuzel, \"{O}ncel},
  doi       = {10.1109/CVPR.2018.00117},
  journal   = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {voxelization, LiDAR, 3D detection, deep learning},
  number    = {1},
  publisher = {IEEE},
  volume    = {2018},
  series    = {CVPR},
  title     = {VoxelNet: End‑to‑End Learning for Point Cloud Based 3D Object Detection},
  url       = {https://arxiv.org/abs/1711.06396},
  year      = {2018}
}

@article{Lang2019PointPillars,
  abstract = {Object detection in point clouds is crucial for robotics applications such as autonomous driving. We introduce PointPillars, a novel encoder that employs PointNets to learn a representation of point clouds organized in vertical columns (pillars). The learned features can be used with any standard 2D convolutional detector; we pair them with a lean downstream network. PointPillars outperforms previous encoders in both speed and accuracy by large margins: it runs at 62 Hz (105 Hz in a faster variant) while achieving state‑of‑the‑art 3D and bird’s‑eye‑view AP on the KITTI benchmark using only LiDAR.},
  author    = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
  doi       = {10.1109/CVPR.2019.00916},
  journal   = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {LiDAR, real‑time detection, 3D object detection, point cloud},
  number    = {1},
  publisher = {IEEE},
  volume    = {2019},
  series    = {CVPR},
  title     = {PointPillars: Fast Encoders for Object Detection from Point Clouds},
  url       = {https://arxiv.org/abs/1812.05784},
  year      = {2019}
}

@article{Shi2019PointRCNN,
  abstract = {We introduce PointRCNN for 3D object detection from raw point clouds. The two‑stage framework first generates bottom‑up 3D proposals by segmenting the whole scene into foreground and background points, then refines each proposal in canonical coordinates by combining local spatial features with global semantic features. Evaluated on the KITTI 3D benchmark, PointRCNN achieves state‑of‑the‑art performance using only point clouds as input, demonstrating the effectiveness of bottom‑up proposal generation and canonical refinement.},
  author    = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  doi       = {10.1109/CVPR.2019.00912},
  journal   = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {3D detection, LiDAR, point cloud segmentation, proposal generation},
  number    = {1},
  publisher = {IEEE},
  volume    = {2019},
  series    = {CVPR},
  title     = {PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud},
  url       = {https://arxiv.org/abs/1812.04244},
  year      = {2019}
}

@article{Shi2020PointGNN,
  abstract = {We propose Point‑GNN, a graph neural network for detecting objects in LiDAR point clouds. Points are encoded into a fixed‑radius nearest‑neighbor graph; Point‑GNN predicts object category and shape for each vertex. An auto‑registration mechanism reduces translation variance, while a box‑merging operation combines detections from multiple vertices. Point‑GNN achieves leading accuracy on KITTI using point cloud alone and even surpasses fusion methods.},
  author    = {Shi, Weijing and Rajkumar, Ragunathan},
  doi       = {10.1109/CVPR42600.2020.00215},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {graph neural networks, 3D detection, point cloud},
  number    = {1},
  publisher = {IEEE},
  volume    = {2020},
  series    = {CVPR},
  title     = {Point‑GNN: Graph Neural Network for 3D Object Detection in a Point Cloud},
  url       = {https://arxiv.org/abs/2003.01251},
  year      = {2020}
}

@article{Shi2020PVRCNN,
  abstract = {We present PointVoxel‑RCNN (PV‑RCNN), a high‑performance 3D object detector that deeply integrates voxel CNNs and PointNet‑based set abstraction to learn discriminative point‑cloud features. PV‑RCNN leverages efficient voxel feature learning for high‑quality proposals and flexible point‑wise receptive fields for accurate refinement. On KITTI and Waymo datasets, it surpasses prior LiDAR‑based methods by significant margins.},
  author    = {Shi, Shaoshuai and Guo, Chaoxu and Jiang, Li and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
  doi       = {10.1109/CVPR42600.2020.00476},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {3D detection, point‑voxel fusion, LiDAR},
  number    = {1},
  publisher = {IEEE},
  volume    = {2020},
  series    = {CVPR},
  title     = {PV‑RCNN: Point‑Voxel Feature Set Abstraction for 3D Object Detection},
  url       = {https://arxiv.org/abs/1912.13192},
  year      = {2020}
}

@article{Vora2020PointPainting,
  abstract = {LiDAR and camera provide complementary information for self‑driving cars, yet LiDAR‑only detectors still dominate benchmarks. We propose PointPainting: a sequential fusion scheme that projects LiDAR points into the output of an image‑only semantic‑segmentation network and appends class scores to each point. The painted point cloud feeds any LiDAR detector. Large improvements are demonstrated on Point‑RCNN, VoxelNet, and PointPillars across KITTI and nuScenes.},
  author    = {Vora, Sourabh and Lang, Alex H. and Helou, Bassam and Beijbom, Oscar},
  doi       = {10.1109/CVPR42600.2020.00538},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {sensor fusion, semantic segmentation, LiDAR, camera},
  number    = {1},
  publisher = {IEEE},
  volume    = {2020},
  series    = {CVPR},
  title     = {PointPainting: Sequential Fusion for 3D Object Detection},
  url       = {https://arxiv.org/abs/1911.10150},
  year      = {2020}
}

@article{Wang2020PseudoLiDAR,
  abstract = {3D object detection is essential for autonomous driving. State‑of‑the‑art accuracy relies on precise but expensive LiDAR sensors, while cheaper monocular or stereo imagery yields far lower accuracies. Contrary to common belief that poor depth estimation is the culprit, we argue that representation is the key. We convert image‑based depth maps into pseudo‑LiDAR points, mimicking LiDAR signals so that off‑the‑shelf LiDAR detectors can be applied. On KITTI, our approach boosts stereo‑image 3D detection accuracy within 30 m from 22 % to 74 %, topping the leaderboard for image‑based methods at submission time.},
  author    = {Wang, Yan and Chao, Wei‑Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
  doi       = {10.1109/CVPR42600.2020.00811},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {pseudo‑LiDAR, stereo depth, monocular depth, 3D detection},
  number    = {1},
  publisher = {IEEE},
  volume    = {2020},
  series    = {CVPR},
  title     = {Pseudo‑LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving},
  url       = {https://arxiv.org/abs/1812.07179},
  year      = {2020}
}

@article{Wang2021FCOS3D,
  abstract = {Monocular 3D object detection is attractive for autonomous driving due to its low cost but is challenging because depth is missing. Building on a fully convolutional single‑stage detector, we propose FCOS3D. By transforming 3D targets into the image domain, distributing objects across feature levels by 2D scale, and redefining center‑ness with a 2D Gaussian around the projected 3D center, FCOS3D becomes a simple yet effective framework without 2D‑3D priors. It ranked 1st among vision‑only methods on the nuScenes 3D detection challenge (NeurIPS 2020).},
  author    = {Wang, Tai and Zhu, Xinge and Pang, Jiangmiao and Lin, Dahua},
  doi       = {10.1109/ICCV48922.2021.00912},
  journal   = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  keywords  = {monocular 3D detection, anchor‑free, autonomous driving},
  number    = {1},
  publisher = {IEEE},
  volume    = {2021},
  series    = {ICCV},
  title     = {FCOS3D: Fully Convolutional One‑Stage Monocular 3D Object Detection},
  url       = {https://arxiv.org/abs/2104.10956},
  year      = {2021}
}
